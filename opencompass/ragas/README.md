# 服务端说明

服务端主要功能包括接收参赛选手的提交，对提交内容进行评测以获取分数，并据此更新排行榜。



## 环境准备

首先请确保您的系统中已安装了 conda，然后通过 conda 创建一个 Python 3.10 环境，并使用 pip 安装所需依赖：


```bash
conda create -n aiops24 python=3.10
conda activate aiops24
pip install -r requirements.txt
```



## 评测脚本使用指南

评测脚本文件名为 `judge.py`。

### 配置文件准备

1. 将 `config/config.example.toml` 文件复制一份并重命名为 `config/config.toml`。
2. 根据配置文件中的注释更新配置项。我们默认使用 OpenAI 的 `gpt-3.5-turbo-16k` 作为评估模型，您也可以选择使用通义千问系列模型 (如 `qwen1.5-72b-chat`)，后者在实测中表现更佳，推荐使用。

### 测试脚本执行

运行以下命令以执行示例评测任务：

```bash
python judge.py -d playground/example
```

### 自定义评测

1. 在 `playground` 目录下创建新的文件夹 (如 `xxx`)，并在其中放置 `ground_truth.jsonl` (参考答案) 和 `result.jsonl`(模型生成的答案)。
2. 执行命令 `python judge.py -d playground/xxx` 来获取评测结果。默认情况下，评测结果将保存在 `playground/xxx/report.json` 中。

脚本参数说明：
* `-d, --directory`：工作目录，用于指定包含参考答案和模型答案的文件夹路径。默认为空，即当前目录。
* `-r, --reference_file`：参考答案文件的路径。默认为 `ground_truth.jsonl`。
* `-a, --answer_file`：模型生成答案文件的路径。默认为 `result.jsonl`。
* `-o, --output`：评测结果将保存在此 JSON 格式的文件中。默认为 `report.json`。

### 文件格式

参考答案文件 (reference file) 和模型生成答案文件 (answer file) 应遵循形如下列示例的格式，至少需要包含 `id`, `query`, `answer` 字段：

```jsonl
{"id": 1, "query": "纯水的化学式是什么？", "answer": "H2O"}
{"id": 2, "query": "一周有多少天？", "answer": "7"}
```

### 评估一致性

若您希望评估机器评分与人工评分的一致性，请在 answer file 的每个 JSON 对象中添加 `label` 字段 (0表示回答错误，1表示回答正确)。脚本将自动检测此字段，计算并输出机器评分与人工评分之间的 Pearson 相关系数、AUC、最佳 F1 分数等指标数据。

### 使用本地部署模型

如果您希望使用自己本地部署的模型作为评估模型，我们建议采用 VLLM 方式部署，并开启 VLLM 的 OpenAI-Compatible Server。这样做可以使您的模型兼容当前评测脚本所使用的 API 调用方式。

#### 配置步骤

1. 部署VLLM模型：按照 VLLM 官方文档的指导部署您的模型，并确保 VLLM 的 OpenAI-Compatible Server 已启动。
2. 修改配置文件：在 `config/config.toml` 文件中，修改以下配置项以指向您的本地模型服务：
    * `openai_api_base`：设置为您的 VLLM OpenAI-Compatible Server 的根 URL（如 `http://localhost:8000/v1`）。
    * `openai_api_key`：填 `EMPTY`。
    * `llm_model`：根据您的 VLLM 配置，设置为相应的模型标识符。

#### 注意事项

* 请确保 VLLM 服务正常运行，并且可以从运行评测脚本的环境中访问到。
* 考虑到网络延迟和性能，本地部署的模型响应时间可能会影响评测脚本的执行效率。



## 评分方式

我们的评分机制综合考虑了**关键词匹配**和**文本相似性**两个维度，以确保评价的全面性和准确性。

### 关键词匹配

在这个部分，我们重点比较模型生成的答案和参考答案中的关键信息点。我们从参考答案中识别出最能反映问题答案主旨的“关键词”，然后检查这些关键词在模型答案中是否出现，以此来评估模型答案的准确性和完整性。

#### 举例说明

> **问题**：计算机网络中的OSI七层模型包括哪些层？
>
> **参考答案**：
>
> OSI七层模型包括：1. 应用层 2. 表示层 3. 会话层 4. 传输层 5. 网络层 6. 数据链路层 7. 物理层
>
> **模型生成的答案**：
>
> OSI模型主要包括两层：
>
> 1. 应用层，是网络模型中与用户最直接相关的一层，负责处理网络应用程序及它们的接口问题；
> 2. 物理层，负责实现与电缆（或其他任何传输媒介）的物理连接，包括定义电缆的电气特性、传输速率、最大传输距离等。

在参考答案中，评估模型会识别到 “应用层”、“表示层”、“会话层”、“传输层”、“网络层”、“数据链路层”、“物理层” 都是关键词，而在模型生成的答案中，评估模型会识别到 “应用层” 和 “物理层” 被提及，与参考答案的关键词相匹配。 因此，在这个例子中，模型答案命中的关键词数量为 2，而参考答案中的关键词总数为 7。该部分得分可以计算如下： 

**关键词得分 = (模型答案命中的关键词数量 / 参考答案中的关键词总数) = (2 / 7) ≈ 0.29**

### 文本相似性

我们基于文本嵌入模型计算文本相似性。具体而言，我们将模型生成的答案和参考答案分别通过一个嵌入模型转换为向量表示，然后计算这两个向量之间的余弦相似度，以评估两个文本之间的相似性。

### 题目得分

对于每道题目，关键词匹配得分和文本相似性得分按照 6:4 的权重加权平均得到本题的分数，计算公式如下：

**题目得分=0.6×关键词得分+0.4×文本相似性得分**

通过这种加权方式，我们既考虑了答案内容的具体匹配程度，也考虑了整体的文本相似性，以实现较为全面的评估。

### 提交得分

队伍的每次提交应包含所有题目的答案，该次提交的得分为所有题目得分的平均值。

### 实现细节

评分机制的详细实现，请参见 `metric.py` 文件，该文件包含了上述评分逻辑的具体代码实现。