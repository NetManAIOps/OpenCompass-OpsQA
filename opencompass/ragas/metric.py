import logging
import difflib
import typing as t
from dataclasses import dataclass, field

import numpy as np

from ragas.llms.json_load import json_loader
from ragas.llms.prompt import Prompt
from ragas.metrics._answer_similarity import AnswerSimilarity
from ragas.metrics.base import EvaluationMode, MetricWithEmbeddings, MetricWithLLM
from ragas.run_config import RunConfig

logger = logging.getLogger(__name__)

CORRECTNESS_PROMPT = Prompt(
    name="answer_correctness",
    instruction="""Given a question, an answer generated by the model, and the ground truth, extract the following information:
            "gt_keywords": Identify and list key terms or phrases from the ground truth that are most relevant to the question. If the ground truth contains multiple parallel items or points, make sure to capture the essence of each item, ensuring that the list comprehensively covers the main aspects or facts presented.
            "overlapping_keywords": From the list of "gt_keywords", identify any terms or phrases that also appear in the model's answer. These overlapping keywords indicate the points of agreement or coverage between the model's answer and the ground truth.
            """,
    examples=[
        {
            "question": """What powers the sun and what is its primary function?""",
            "ground_truth": """The sun is actually powered by nuclear fusion, not fission. In its core, hydrogen atoms fuse to form helium, releasing a tremendous amount of energy. This energy is what lights up the sun and provides heat and light, essential for life on Earth. The sun's light also plays a critical role in Earth's climate system and helps to drive the weather and ocean currents.""",
            "answer": """The sun is powered by nuclear fission, similar to nuclear reactors on Earth, and its primary function is to provide light to the solar system.""",
            "Extracted statements": {
                "gt_keywords": ["nuclear fusion", "light"],
                "overlapping_keywords": ["light"],
            },
        },
        {
            "question": """What is the boiling point of water?""",
            "ground_truth": """The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit) at sea level, but it can change with altitude.""",
            "answer": """The boiling point of water is 100 degrees Celsius at sea level.""",
            "Extracted statements": {
                "gt_keywords": ["100 degrees Celsius"],
                "overlapping_keywords": ["100 degrees Celsius"],
            },
        },
        {
            "question": "What information should be submitted when contacting technical support for a communication technology company?",
            "ground_truth": "When contacting technical support for a communication technology company, the following information should be provided: 1. Fault details: Time, location, and event description. 2. Log files and alarm query results. 3. Steps taken to address the issue, commands executed, and the results of those actions. 4. Remote access details and contact information for relevant personnel.",
            "answer": "When contacting technical support for a communication technology company, the following information should be provided: 1. Fault details: Time, location, and event description.",
            "Extracted statements": {
                "gt_keywords": ["fault details", "log files and alarm query results", "steps taken and commands executed", "remote access details and contact information for relevant personnel"],
                "overlapping_keywords": ["fault details"]
            },
        },
    ],
    input_keys=["question", "ground_truth", "answer"],
    output_key="Extracted statements",
    output_type="json",
)


@dataclass
class AnswerCorrectness(MetricWithLLM, MetricWithEmbeddings):

    """
    Measures answer correctness compared to ground truth as a combination of
    factuality and semantic similarity.

    Attributes
    ----------
    name: string
        The name of the metrics
    weights:
        a list of two weights corresponding to factuality and semantic similarity
        Defaults [0.6, 0.4]
    answer_similarity:
        The AnswerSimilarity object
    """

    name: str = "answer_correctness"  # type: ignore[reportIncompatibleMethodOverride]
    evaluation_mode: EvaluationMode = EvaluationMode.qga  # type: ignore[reportIncompatibleMethodOverride]
    correctness_prompt: Prompt = field(default_factory=lambda: CORRECTNESS_PROMPT)
    weights: list[float] = field(default_factory=lambda: [0.6, 0.4])
    keyword_matching_threshold: float = field(default_factory=lambda: 0.7)
    answer_similarity: AnswerSimilarity | None = None

    def __post_init__(self):
        if len(self.weights) != 2:
            raise ValueError(
                "Expects a list of two weights. First for factuality, second for semantic similarity"
            )
        if all([w == 0 for w in self.weights]):
            raise ValueError("At least one weight must be non-zero")
        if not all([w >= 0 for w in self.weights]):
            raise ValueError("Weights must be non-negative")

    def init(self, run_config: RunConfig):
        super().init(run_config)
        if self.answer_similarity is None and self.weights[1] != 0:
            self.answer_similarity = AnswerSimilarity(
                llm=self.llm, embeddings=self.embeddings
            )

    def _compute_statement_presence(self, prediction: t.Any) -> float:
        assert self.llm is not None, "LLM must be set"

        key_map = [
            "gt_keywords",
            "overlapping_keywords",
        ]
        if prediction:
            prediction = [prediction.get(k, np.nan) for k in key_map]
            gt_keywords, overlapping_keywords = [
                item if isinstance(item, list) else np.nan for item in prediction
            ]
            if gt_keywords is None or (type(gt_keywords) == float and np.isnan(gt_keywords)):
                logger.warning('[gt_keywords] gt_keywords is nan!')
                gt_keywords = []
            logger.warning(f"[gt_keywords] {gt_keywords}")
            gt_keywords = [k.lower() for k in gt_keywords]
            overlapping_keywords = [k.lower() for k in overlapping_keywords]
            overlapping_keywords = [k for k in overlapping_keywords if self.match(gt_keywords, k)]
            num_ok = len(overlapping_keywords)
            num_all = len(gt_keywords)
            if any([np.isnan(i) for i in [num_ok, num_all]]):
                score = np.nan
                logger.warning(
                    "Invalid prediction format. Expected a list of dictionaries with keys 'gt_keywords', 'overlapping_keywords'"
                )
            else:
                score = min(num_ok / num_all, 1.0) if num_all > 0 else np.nan
        else:
            score = np.nan

        return score

    def match(self, arr, k):
        for item in arr:
            if difflib.SequenceMatcher(None, item, k).ratio() >= self.keyword_matching_threshold:
                return True
        return False

    async def _ascore(self, row: dict, callbacks: t.Any, is_async: bool) -> float:
        assert self.llm is not None, "LLM must be set"

        q, a, g = row["question"], row["answer"], row["ground_truth"]
        p_value = self.correctness_prompt.format(question=q, ground_truth=g, answer=a)

        # TODO: add chat_template
        p_value.prompt_str = '<|im_start|>system\nYou are a helpful assistant.<|im_end|><|im_start|>user\n' + p_value.prompt_str + '<|im_end|><|im_start|>assistant\n'

        is_statement_present = await self.llm.generate(
            p_value, callbacks=callbacks, is_async=is_async, stop=['<|im_end|>', '<|endoftext|>']
        )

        prediction = await json_loader.safe_load(
            is_statement_present.generations[0][0].text, self.llm, is_async=is_async
        )
        logger.warning(f"\n-------------------------------------\n[prompt] {p_value}\n[prediction] {prediction}")
        f1_score = self._compute_statement_presence(prediction)

        if self.weights[1] == 0:
            similarity_score = 0
        else:
            assert self.answer_similarity is not None, "AnswerSimilarity must be set"

            callbacks = []
            
            similarity_score = await self.answer_similarity.ascore(
                row, callbacks=callbacks, is_async=is_async
            )

        score = np.average(
            [f1_score, similarity_score],
            weights=self.weights,
        )

        return float(score)

    def adapt(self, language: str, cache_dir: t.Optional[str] = None) -> None:
        assert self.llm is not None, "llm must be set to compute score"

        logger.info(f"Adapting AnswerCorrectness metric to {language}")
        self.correctness_prompt = self.correctness_prompt.adapt(
            language, self.llm, cache_dir
        )

    def save(self, cache_dir: t.Optional[str] = None) -> None:
        self.correctness_prompt.save(cache_dir)


answer_correctness = AnswerCorrectness()
